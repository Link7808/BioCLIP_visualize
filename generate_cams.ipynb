{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang.13617/miniconda3/envs/gscorecam/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import open_clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_grad_cam.utils.image import scale_cam_image\n",
    "from utils import parse_xml_to_dict, scoremap2bbox\n",
    "from clip_text import class_names#, imagenet_templates\n",
    "import argparse\n",
    "from torch import multiprocessing\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip\n",
    "\n",
    "import types\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import open_clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_grad_cam.utils.image import scale_cam_image\n",
    "from utils import parse_xml_to_dict, scoremap2bbox\n",
    "from clip_text import class_names#, imagenet_templates\n",
    "import argparse\n",
    "from torch import multiprocessing\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip\n",
    "\n",
    "import types\n",
    "import sys\n",
    "sys.path.insert(0, '/home/zhang.13617/Desktop/BioCLIP_visualize')\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def reshape_transform(tensor, height=None, width=None):\n",
    "    tensor = tensor.permute(1, 0, 2)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "    seq_len = tensor.size(1)\n",
    "    batch_size = tensor.size(0)\n",
    "    hidden_dim = tensor.size(2)\n",
    "    n_tokens = seq_len - 1  # Exclude the first token (e.g., [CLS] token)\n",
    "\n",
    "    if height is not None and width is not None:\n",
    "        if height * width != n_tokens:\n",
    "            raise ValueError(f\"Provided height ({height}) and width ({width}) do not multiply to n_tokens ({n_tokens})\")\n",
    "    elif height is not None:\n",
    "        if n_tokens % height != 0:\n",
    "            raise ValueError(f\"Cannot reshape: n_tokens ({n_tokens}) is not divisible by provided height ({height})\")\n",
    "        width = n_tokens // height\n",
    "    elif width is not None:\n",
    "        if n_tokens % width != 0:\n",
    "            raise ValueError(f\"Cannot reshape: n_tokens ({n_tokens}) is not divisible by provided width ({width})\")\n",
    "        height = n_tokens // width\n",
    "    else:\n",
    "        # Automatically determine height and width\n",
    "        height = int(n_tokens ** 0.5)\n",
    "        while n_tokens % height != 0 and height > 1:\n",
    "            height -= 1\n",
    "        if height == 1 and n_tokens % height != 0:\n",
    "            raise ValueError(f\"Cannot automatically determine height and width for n_tokens ({n_tokens})\")\n",
    "        width = n_tokens // height\n",
    "\n",
    "    result = tensor[:, 1:, :].reshape(batch_size, height, width, hidden_dim)\n",
    "    result = result.permute(0, 3, 1, 2)  # Shape: (batch_size, hidden_dim, height, width)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def zeroshot_classifier(classnames, templates, model):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in classnames:\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = tokenizer(texts).to(device) #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)\n",
    "    return zeroshot_weights.t()\n",
    "\n",
    "class ClipOutputTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "    def __call__(self, model_output):\n",
    "        if len(model_output.shape) == 1:\n",
    "            return model_output[self.category]\n",
    "        return model_output[:, self.category]\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform_resize(h, w):\n",
    "    return Compose([\n",
    "        Resize((h,w), interpolation=BICUBIC),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "def img_ms_and_flip(img_path, ori_height, ori_width, scales=[1.0], patch_size=16):\n",
    "    all_imgs = []\n",
    "    for scale in scales:\n",
    "        preprocess = _transform_resize(int(np.ceil(scale * int(ori_height) / patch_size) * patch_size), int(np.ceil(scale * int(ori_width) / patch_size) * patch_size))\n",
    "        image = preprocess(Image.open(img_path))\n",
    "        image_ori = image\n",
    "        image_flip = torch.flip(image, [-1])\n",
    "        all_imgs.append(image_ori)\n",
    "        all_imgs.append(image_flip)\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def adjust_image_size(h, w, n_tokens, patch_size):\n",
    "    import math\n",
    "    \n",
    "    \n",
    "    aspect_ratio = w / h\n",
    "    \n",
    "    h_patches = int(round(math.sqrt(n_tokens / aspect_ratio)))\n",
    "    h_patches = max(h_patches, 1)  \n",
    "    w_patches = n_tokens // h_patches\n",
    "    \n",
    "    while h_patches * w_patches != n_tokens:\n",
    "        if h_patches * w_patches < n_tokens:\n",
    "            w_patches += 1\n",
    "        else:\n",
    "            h_patches -= 1\n",
    "\n",
    "    \n",
    "    adjusted_height = h_patches * patch_size\n",
    "    adjusted_width = w_patches * patch_size\n",
    "    \n",
    "    return adjusted_height, adjusted_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfrom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def perform(dataset_list, args, model, fg_text_features, cam):\n",
    "\n",
    "    device_id = \"cuda:6\"\n",
    "    device = torch.device(device_id)\n",
    "    model = model.to(device)\n",
    "    fg_text_features = fg_text_features.to(device)\n",
    "\n",
    "    os.makedirs(args.cam_out_dir, exist_ok=True)\n",
    "\n",
    "    for im_idx, img_path in enumerate(tqdm(dataset_list)):\n",
    "        image_filename = os.path.basename(img_path)\n",
    "        image = Image.open(img_path).convert(\"RGB\")  \n",
    "        ori_width, ori_height = image.size \n",
    "\n",
    "        adjusted_height, adjusted_width = adjust_image_size(ori_width, ori_height, 660, patch_size=16)\n",
    "        processed_image = _transform_resize(adjusted_height, adjusted_width)(Image.open(img_path))\n",
    "        ms_imgs = [processed_image]  \n",
    "\n",
    "\n",
    "        for aug_idx, img_tensor in enumerate(ms_imgs):\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)  \n",
    "\n",
    "            h, w = img_tensor.shape[-2], img_tensor.shape[-1]\n",
    "\n",
    "            image_features, attn_weight_list = model.encode_image(img_tensor, h, w)\n",
    "\n",
    "        \n",
    "            input_tensor = [image_features, fg_text_features, h, w]\n",
    "\n",
    "            targets = None\n",
    "\n",
    "            grayscale_cam, logits_per_image, attn_weight_last = cam(input_tensor=input_tensor,\n",
    "                                                                    targets=targets,\n",
    "                                                                    target_size=None)  \n",
    "            grayscale_cam = grayscale_cam[0, :]  # 形状：[H, W]\n",
    "\n",
    "            grayscale_cam_highres = cv2.resize(grayscale_cam, (ori_width, ori_height))\n",
    "            highres_cam = torch.tensor(grayscale_cam_highres)\n",
    "\n",
    "\n",
    "\n",
    "            if not attn_weight_list:\n",
    "                raise ValueError(\"attn_weight_list is empty.\")\n",
    "\n",
    "          \n",
    "            attn_weight = [aw[:, 1:, 1:] for aw in attn_weight_list]  \n",
    "            attn_weight = torch.stack(attn_weight, dim=0)[-8:] \n",
    "            attn_weight = torch.mean(attn_weight, dim=0)\n",
    "            attn_weight = attn_weight[0].cpu().detach().float()  \n",
    "\n",
    "            box, cnt = scoremap2bbox(scoremap=grayscale_cam, threshold=0.4, multi_contour_eval=True)\n",
    "\n",
    "            aff_mask = torch.zeros((grayscale_cam.shape[0], grayscale_cam.shape[1]))\n",
    "            for i_ in range(cnt):\n",
    "                x0_, y0_, x1_, y1_ = box[i_]\n",
    "                aff_mask[y0_:y1_, x0_:x1_] = 1\n",
    "\n",
    "            aff_mask = aff_mask.view(1, -1)  #\n",
    "            aff_mat = attn_weight  #\n",
    "\n",
    "            trans_mat = aff_mat / torch.sum(aff_mat, dim=0, keepdim=True)\n",
    "            trans_mat = trans_mat / torch.sum(trans_mat, dim=1, keepdim=True)\n",
    "\n",
    "            for _ in range(2):\n",
    "                trans_mat = trans_mat / torch.sum(trans_mat, dim=0, keepdim=True)\n",
    "                trans_mat = trans_mat / torch.sum(trans_mat, dim=1, keepdim=True)\n",
    "            trans_mat = (trans_mat + trans_mat.transpose(1, 0)) / 2\n",
    "\n",
    "            for _ in range(1):\n",
    "                trans_mat = torch.matmul(trans_mat, trans_mat)\n",
    "\n",
    "            trans_mat = trans_mat * aff_mask\n",
    "\n",
    "            cam_to_refine = torch.FloatTensor(grayscale_cam).view(-1, 1)  # 形状：[n, 1]\n",
    "\n",
    "            cam_refined = torch.matmul(trans_mat, cam_to_refine).reshape(h // 16, w // 16)\n",
    "            cam_refined = cam_refined.cpu().numpy().astype(np.float32)\n",
    "\n",
    "            cam_refined_highres = scale_cam_image([cam_refined], (ori_width, ori_height))[0]\n",
    "\n",
    "            output_filename = image_filename.replace('jpg', 'npy')\n",
    "\n",
    "            np.save(os.path.join(args.cam_out_dir, output_filename),\n",
    "                    {\n",
    "                        \"highres_cam\": grayscale_cam_highres.astype(np.float32),\n",
    "                        \"attn_highres\": cam_refined_highres.astype(np.float32),\n",
    "                    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[70520]\n",
      "loss is \n",
      "tensor(32.4801, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:01<00:08,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[368783]\n",
      "loss is \n",
      "tensor(30.4969, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:05,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[368783]\n",
      "loss is \n",
      "tensor(31.3628, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:02<00:03,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[368783]\n",
      "loss is \n",
      "tensor(31.4413, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:02<00:02,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[71416]\n",
      "loss is \n",
      "tensor(30.4425, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:03<00:01,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[349279]\n",
      "loss is \n",
      "tensor(30.2446, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:03<00:01,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[349279]\n",
      "loss is \n",
      "tensor(31.8274, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:04<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_cate is \n",
      "[65872]\n",
      "loss is \n",
      "tensor(31.6953, device='cuda:6', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('--split_file', type=str, default='/home/zhang.13617/Desktop/datas/red_winged_black/l.txt')\n",
    "    parser.add_argument('--cam_out_dir', type=str, default='/home/zhang.13617/Desktop/BioCLIP_visualize/cams')\n",
    "    args, unknown = parser.parse_known_args()  \n",
    "\n",
    "    device = \"cuda:6\"\n",
    "\n",
    "    txt_emb_npy = \"/home/zhang.13617/Desktop/datas/txt_emb_species.npy\"\n",
    "    fg_text_features = torch.from_numpy(np.load(txt_emb_npy, mmap_mode=\"r\")).to(device)\n",
    "    fg_text_features = fg_text_features.t()\n",
    "\n",
    "\n",
    "\n",
    "    print(device)\n",
    "    with open(args.split_file, 'r') as file:\n",
    "        train_list = [line.strip() for line in file] \n",
    "    train_list = [line.strip() for line in train_list]\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    tokenizer = open_clip.get_tokenizer('hf-hub:imageomics/bioclip')\n",
    "\n",
    "    #fg_text_features = zeroshot_classifier(class_names, ['a clean origami {}.'], model)\n",
    "\n",
    "    target_layers = [model.visual.transformer.resblocks[-1].ln_1]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)\n",
    "\n",
    "    perform(train_list, args, model, fg_text_features, cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overlay_rgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, output_filename)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Call main function to visualize CAM on image\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcam_npy_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cam_npy_path, img_path, output_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m         visualize_cam_on_image(img, cam_single, output_path_idx)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mvisualize_cam_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mvisualize_cam_on_image\u001b[0;34m(img, cam, output_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m overlay \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(img, \u001b[38;5;241m0.5\u001b[39m, heatmap, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Convert overlay from BGR to RGB for visualization with matplotlib\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Display overlay image\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43moverlay_rgb\u001b[49m)\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overlay_rgb' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cam_on_image(img, cam, output_path=None):\n",
    "    # Normalize CAM\n",
    "    cam = cam - np.min(cam)\n",
    "    cam = cam / np.max(cam)\n",
    "\n",
    "    # Convert cam to uint8 type, range [0, 255]\n",
    "    cam = np.uint8(255 * cam)\n",
    "\n",
    "    # Create heatmap\n",
    "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Ensure heatmap and img have the same size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Overlay heatmap on original image\n",
    "    overlay = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "    # Convert overlay from BGR to RGB for visualization with matplotlib\n",
    "\n",
    "    # Display overlay image\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save overlay image if output_path is provided\n",
    "    if output_path:\n",
    "        # Convert overlay back to RGB before saving to ensure correct color order\n",
    "        overlay_rgb_for_save = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(output_path, overlay_rgb_for_save)\n",
    "\n",
    "def main(cam_npy_path, img_path, output_path=None):\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image {img_path}\")\n",
    "        return\n",
    "\n",
    "    # Read CAM data\n",
    "    try:\n",
    "        cam_data = np.load(cam_npy_path, allow_pickle=True).item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CAM npy file: {cam_npy_path}, error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Check if 'attn_highres' exists in the loaded npy data\n",
    "    if \"attn_highres\" not in cam_data:\n",
    "        print(f\"Error: 'attn_highres' not found in {cam_npy_path}\")\n",
    "        return\n",
    "\n",
    "    cam = cam_data.get(\"attn_highres\")  # Get 'attn_highres' from npy file\n",
    "\n",
    "    # Ensure 'cam' is valid\n",
    "    if cam is None:\n",
    "        print(f\"Error: 'attn_highres' is None in {cam_npy_path}\")\n",
    "        return\n",
    "\n",
    "    # If cam is a 3D array (multiple heatmaps), process each one\n",
    "    if cam.ndim == 3:\n",
    "        for idx in range(cam.shape[0]):\n",
    "            cam_single = cam[idx]\n",
    "            # Adjust output filename to include index\n",
    "            output_path_idx = output_path.replace('.jpg', f'_{idx}.jpg')\n",
    "            visualize_cam_on_image(img, cam_single, output_path_idx)\n",
    "    else:\n",
    "        visualize_cam_on_image(img, cam, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cam_npy_dir = \"/home/zhang.13617/Desktop/BioCLIP_visualize/cams\"\n",
    "    train_absolute_txt = \"/home/zhang.13617/Desktop/datas/red_winged_black/l.txt\"\n",
    "    output_dir = \"/home/zhang.13617/Desktop/BioCLIP_visualize/cams\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get paths to .npy files sorted by filename\n",
    "    cam_npy_paths = sorted([os.path.join(cam_npy_dir, f) for f in os.listdir(cam_npy_dir) if f.endswith('.npy')])\n",
    "\n",
    "    # Read image absolute paths from train_absolute.txt\n",
    "    with open(train_absolute_txt, 'r') as f:\n",
    "        img_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Iterate over CAM npy paths (process only available CAM files)\n",
    "    for cam_npy_path in cam_npy_paths:\n",
    "        # Try to find the corresponding image in img_paths by matching the base filename\n",
    "        cam_filename = os.path.basename(cam_npy_path).replace('.npy', '')\n",
    "        matching_img_paths = [img_path for img_path in img_paths if cam_filename in os.path.basename(img_path)]\n",
    "        \n",
    "        if not matching_img_paths:\n",
    "            print(f\"No matching image found for CAM file: {cam_npy_path}\")\n",
    "            continue\n",
    "\n",
    "        # Use the first matching image path\n",
    "        img_path = matching_img_paths[0]\n",
    "        base_name = os.path.basename(img_path).replace('.jpg', '')\n",
    "\n",
    "        # Generate output path\n",
    "        output_filename = base_name + f'_output.jpg'\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        # Call main function to visualize CAM on image\n",
    "        main(cam_npy_path, img_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gscorecam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
